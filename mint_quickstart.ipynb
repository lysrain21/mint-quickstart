{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# MinT Quickstart Guide\n",
    "\n",
    "## What is MinT?\n",
    "\n",
    "**MinT (Mind Lab Toolkit)** is an open infrastructure for training language models using:\n",
    "- **SFT (Supervised Fine-Tuning)**: Learn from labeled examples (input → output pairs)\n",
    "- **RL (Reinforcement Learning)**: Learn from reward signals (trial and error)\n",
    "\n",
    "MinT uses **LoRA (Low-Rank Adaptation)** to efficiently fine-tune large models without modifying all parameters.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, we'll train a model to solve **multiplication problems** using a two-stage approach:\n",
    "\n",
    "1. **Stage 1 (SFT)**: Teach the model multiplication with labeled examples\n",
    "2. **Stage 2 (RL)**: Load the SFT model and refine it with reward signals\n",
    "\n",
    "This demonstrates the complete workflow: SFT → Save → Load → RL\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python >= 3.11\n",
    "- A MinT API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Installation\n",
    "\n",
    "Install the MinT SDK from the git repository (alpha - API may change):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/MindLab-Research/mindlab-toolkit.git python-dotenv matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apikey-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configure Your API Key\n",
    "\n",
    "MinT requires an API key for authentication. There are two ways to set it up:\n",
    "\n",
    "### Option A: Using a `.env` file (Recommended)\n",
    "\n",
    "Create a file named `.env` in your project directory:\n",
    "\n",
    "```\n",
    "MINT_API_KEY=sk-mint-your-api-key-here\n",
    "```\n",
    "\n",
    "### Option B: Set environment variable directly\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['MINT_API_KEY'] = 'sk-mint-your-api-key-here'\n",
    "```\n",
    "\n",
    "**Security Note**: Never commit your API key to version control. Add `.env` to your `.gitignore` file.\n",
    "\n",
    "---\n",
    "\n",
    "### Tinker Compatibility\n",
    "\n",
    "MinT is fully API-compatible with [Tinker](https://tinker.thinkingmachines.ai). If you prefer, you can use the `tinker` package with MinT by configuring environment variables to point to the MinT server:\n",
    "\n",
    "```bash\n",
    "pip install tinker\n",
    "```\n",
    "\n",
    "```\n",
    "TINKER_BASE_URL=https://mint.macaron.im/\n",
    "TINKER_API_KEY=<your-mint-api-key>\n",
    "```\n",
    "\n",
    "All code in this tutorial works identically with `import tinker` instead of `import mint`.\n",
    "\n",
    "---\n",
    "\n",
    "### HuggingFace Mirror\n",
    "\n",
    "If you have network issues accessing HuggingFace, set the mirror endpoint **before** importing `mint`:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "import mint  # Must be after setting HF_ENDPOINT\n",
    "```\n",
    "\n",
    "This redirects all HuggingFace model downloads to the mirror site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.environ.get('MINT_API_KEY'):\n",
    "    print(\"WARNING: MINT_API_KEY not found!\")\n",
    "    print(\"Please create a .env file with: MINT_API_KEY=sk-mint-your-key-here\")\n",
    "else:\n",
    "    print(\"API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Connect to MinT Server\n",
    "\n",
    "The `ServiceClient` is your entry point to MinT. It handles:\n",
    "- Authentication with the server\n",
    "- Creating training and sampling clients\n",
    "- Querying available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mint\n",
    "\n",
    "# Create the service client\n",
    "service_client = mint.ServiceClient()\n",
    "\n",
    "# List available models\n",
    "print(\"Connected to MinT server!\")\n",
    "print(\"\\nAvailable models:\")\n",
    "try:\n",
    "    capabilities = service_client.get_server_capabilities()\n",
    "    for model in capabilities.supported_models:\n",
    "        print(f\"  - {model.model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concepts-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Core Concepts\n",
    "\n",
    "Before we start training, let's understand the key concepts:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|:--------|\n",
    "| `TrainingClient` | Manages your LoRA adapter and handles training operations |\n",
    "| `SamplingClient` | Generates text from your trained model |\n",
    "| `Datum` | A single training example with `model_input` and `loss_fn_inputs` |\n",
    "\n",
    "### Training Loop Pattern\n",
    "```python\n",
    "for each batch:\n",
    "    forward_backward()  # Compute gradients\n",
    "    optim_step()        # Update model weights\n",
    "```\n",
    "\n",
    "### Loss Functions\n",
    "- **cross_entropy**: For SFT - maximizes probability of correct tokens\n",
    "- **importance_sampling**: For RL - weights updates by advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sft-header-md",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 1: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "**Goal**: Teach the model to solve two-digit multiplication using labeled examples.\n",
    "\n",
    "**How SFT works**:\n",
    "1. Show the model input-output pairs\n",
    "2. Model learns to predict the output given the input\n",
    "3. Loss = how surprised the model is by the correct answer\n",
    "\n",
    "```\n",
    "Input:  \"Question: What is 47 * 83?\\nAnswer:\"\n",
    "Output: \" 3901\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-training-md",
   "metadata": {},
   "source": [
    "### Step 3: Create a Training Client\n",
    "\n",
    "We'll use `Qwen/Qwen3-0.6B` - a small but capable model perfect for learning.\n",
    "\n",
    "**LoRA Parameters**:\n",
    "- `rank=16`: Size of the low-rank matrices (higher = more capacity, slower)\n",
    "- `train_mlp=True`: Train the feed-forward layers\n",
    "- `train_attn=True`: Train the attention layers\n",
    "- `train_unembed=True`: Train the output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-training-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mint import types\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# Create a training client with LoRA configuration\n",
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=BASE_MODEL,\n",
    "    rank=16,              # LoRA rank - controls adapter capacity\n",
    "    train_mlp=True,       # Train MLP (feed-forward) layers\n",
    "    train_attn=True,      # Train attention layers  \n",
    "    train_unembed=True,   # Train the output projection\n",
    ")\n",
    "print(f\"Training client created for: {BASE_MODEL}\")\n",
    "\n",
    "# Get the tokenizer - converts text to/from token IDs\n",
    "tokenizer = training_client.get_tokenizer()\n",
    "print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-md",
   "metadata": {},
   "source": [
    "### Step 4: Prepare Training Data\n",
    "\n",
    "We need to convert our examples into `Datum` objects that MinT can process.\n",
    "\n",
    "**Key concept - Weights**:\n",
    "- `weight=0`: Don't compute loss on this token (the prompt)\n",
    "- `weight=1`: Compute loss on this token (the answer we want to learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate arithmetic training examples\n",
    "import random\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "def generate_sft_examples(n=100):\n",
    "    \"\"\"Generate two-digit multiplication examples for SFT.\"\"\"\n",
    "    examples = []\n",
    "    for _ in range(n):\n",
    "        a = random.randint(10, 99)\n",
    "        b = random.randint(10, 99)\n",
    "        examples.append({\n",
    "            \"question\": f\"What is {a} * {b}?\",\n",
    "            \"answer\": str(a * b)\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "sft_examples = generate_sft_examples(100)\n",
    "\n",
    "print(f\"Generated {len(sft_examples)} training examples\")\n",
    "print(\"\\nSample examples:\")\n",
    "for ex in sft_examples[:3]:\n",
    "    print(f\"  Q: {ex['question']} → A: {ex['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sft_example(example: dict, tokenizer) -> types.Datum:\n",
    "    \"\"\"\n",
    "    Convert a training example into a Datum for MinT.\n",
    "    \n",
    "    The model learns next-token prediction:\n",
    "    Given tokens [0, 1, 2, 3], predict [1, 2, 3, 4]\n",
    "    \n",
    "    We use weights to only compute loss on the answer tokens.\n",
    "    \"\"\"\n",
    "    # Format the prompt and completion\n",
    "    prompt = f\"Question: {example['question']}\\nAnswer:\"\n",
    "    completion = f\" {example['answer']}\"\n",
    "    \n",
    "    # Tokenize prompt and completion separately\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    completion_tokens = tokenizer.encode(completion, add_special_tokens=False)\n",
    "    \n",
    "    # Add EOS token so model learns when to stop\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    completion_tokens = completion_tokens + [eos_token_id]\n",
    "    \n",
    "    # Create weights: 0 for prompt (don't learn), 1 for completion (learn this)\n",
    "    prompt_weights = [0] * len(prompt_tokens)\n",
    "    completion_weights = [1] * len(completion_tokens)\n",
    "    \n",
    "    # Combine everything\n",
    "    all_tokens = prompt_tokens + completion_tokens\n",
    "    all_weights = prompt_weights + completion_weights\n",
    "    \n",
    "    # For next-token prediction:\n",
    "    # - input_tokens:  all tokens except the last\n",
    "    # - target_tokens: all tokens except the first (shifted by 1)\n",
    "    input_tokens = all_tokens[:-1]\n",
    "    target_tokens = all_tokens[1:]\n",
    "    weights = all_weights[1:]  # Weights align with targets\n",
    "    \n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs={\n",
    "            \"target_tokens\": target_tokens,\n",
    "            \"weights\": weights\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Process all examples\n",
    "sft_data = [process_sft_example(ex, tokenizer) for ex in sft_examples]\n",
    "print(f\"Prepared {len(sft_data)} training datums\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-md",
   "metadata": {},
   "source": [
    "### Visualize the Data\n",
    "\n",
    "Let's see how the first example looks after processing.\n",
    "- **Weight=0**: Prompt tokens (model sees these but doesn't learn from them)\n",
    "- **Weight=1**: Completion tokens (model learns to predict these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first example\n",
    "datum0 = sft_data[0]\n",
    "\n",
    "print(f\"Example: Q: {sft_examples[0]['question']} → A: {sft_examples[0]['answer']}\")\n",
    "print(f\"EOS token: {repr(tokenizer.decode([tokenizer.eos_token_id]))}\")\n",
    "print()\n",
    "print(f\"{'Input Token':<20} {'Target Token':<20} {'Weight':<10} {'Learn?'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "inputs = datum0.model_input.to_ints()\n",
    "targets = datum0.loss_fn_inputs['target_tokens']\n",
    "weights = datum0.loss_fn_inputs['weights']\n",
    "\n",
    "# Handle both list and tensor types\n",
    "if hasattr(targets, 'tolist'):\n",
    "    targets = targets.tolist()\n",
    "if hasattr(weights, 'tolist'):\n",
    "    weights = weights.tolist()\n",
    "\n",
    "for inp, tgt, wgt in zip(inputs, targets, weights):\n",
    "    learn = \"YES\" if wgt > 0 else \"no\"\n",
    "    inp_str = repr(tokenizer.decode([inp]))\n",
    "    tgt_str = repr(tokenizer.decode([tgt]))\n",
    "    print(f\"{inp_str:<20} {tgt_str:<20} {wgt:<10} {learn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sft-train-md",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model (SFT)\n",
    "\n",
    "The training loop:\n",
    "1. **forward_backward()**: Compute the loss and gradients\n",
    "2. **optim_step()**: Update model weights using Adam optimizer\n",
    "\n",
    "We'll run multiple steps and watch the loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sft-train-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training configuration\n",
    "NUM_SFT_STEPS = 10\n",
    "SFT_LEARNING_RATE = 5e-5\n",
    "\n",
    "sft_losses = []\n",
    "print(f\"Starting SFT training for {NUM_SFT_STEPS} steps...\")\n",
    "print(f\"Learning rate: {SFT_LEARNING_RATE}\")\n",
    "print()\n",
    "\n",
    "for step in range(NUM_SFT_STEPS):\n",
    "    fwdbwd_future = training_client.forward_backward(\n",
    "        data=sft_data,\n",
    "        loss_fn=\"cross_entropy\"\n",
    "    )\n",
    "    fwdbwd_result = fwdbwd_future.result()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_weight = 0.0\n",
    "    for i, out in enumerate(fwdbwd_result.loss_fn_outputs):\n",
    "        logprobs = out['logprobs']\n",
    "        if hasattr(logprobs, 'tolist'):\n",
    "            logprobs = logprobs.tolist()\n",
    "        w = sft_data[i].loss_fn_inputs['weights']\n",
    "        if hasattr(w, 'tolist'):\n",
    "            w = w.tolist()\n",
    "        for lp, wt in zip(logprobs, w):\n",
    "            total_loss += -lp * wt\n",
    "            total_weight += wt\n",
    "    \n",
    "    loss = total_loss / max(total_weight, 1)\n",
    "    sft_losses.append(loss)\n",
    "    \n",
    "    optim_future = training_client.optim_step(\n",
    "        types.AdamParams(learning_rate=SFT_LEARNING_RATE)\n",
    "    )\n",
    "    optim_future.result()\n",
    "    \n",
    "    print(f\"Step {step+1:2d}/{NUM_SFT_STEPS}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nSFT training complete!\")\n",
    "print(f\"Loss: {sft_losses[0]:.4f} → {sft_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sft-sample-md",
   "metadata": {},
   "source": [
    "### Step 6: Test the SFT Model\n",
    "\n",
    "Let's see if our model learned basic arithmetic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sft-sample-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(response: str) -> str | None:\n",
    "    \"\"\"Extract the first numeric answer from response.\"\"\"\n",
    "    numbers = re.findall(r'\\d+', response)\n",
    "    return numbers[0] if numbers else None\n",
    "\n",
    "# Save weights and create sampling client\n",
    "print(\"Saving SFT model weights...\")\n",
    "sft_sampling_client = training_client.save_weights_and_get_sampling_client(\n",
    "    name='arithmetic-sft'\n",
    ")\n",
    "print(\"Model ready for inference!\\n\")\n",
    "\n",
    "# Test on problems in SFT range (10-99)\n",
    "test_problems = [\n",
    "    (\"What is 23 * 47?\", \"1081\"),\n",
    "    (\"What is 56 * 34?\", \"1904\"),\n",
    "    (\"What is 71 * 89?\", \"6319\"),\n",
    "]\n",
    "\n",
    "print(\"Testing SFT model (10-99 range):\")\n",
    "print(\"=\" * 50)\n",
    "sft_correct = 0\n",
    "\n",
    "for question, correct in test_problems:\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    prompt_tokens = types.ModelInput.from_ints(tokenizer.encode(prompt))\n",
    "    \n",
    "    result = sft_sampling_client.sample(\n",
    "        prompt=prompt_tokens,\n",
    "        num_samples=1,\n",
    "        sampling_params=types.SamplingParams(\n",
    "            max_tokens=16,\n",
    "            temperature=0.0,\n",
    "            stop_token_ids=[tokenizer.eos_token_id]\n",
    "        )\n",
    "    ).result()\n",
    "    \n",
    "    response = tokenizer.decode(result.sequences[0].tokens)\n",
    "    extracted = extract_answer(response)\n",
    "    is_correct = extracted == correct\n",
    "    if is_correct:\n",
    "        sft_correct += 1\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response.strip()} (extracted: {extracted}, correct: {correct}) [{'PASS' if is_correct else 'FAIL'}]\")\n",
    "    print()\n",
    "\n",
    "print(f\"SFT Accuracy: {sft_correct}/{len(test_problems)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-checkpoint-md",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 7: Save Checkpoint\n",
    "\n",
    "Save the SFT model so we can load it and continue training with RL.\n",
    "\n",
    "**`save_state()`** saves:\n",
    "- LoRA weights\n",
    "- Optimizer state (for seamless training continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-checkpoint-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full training state (weights + optimizer)\n",
    "sft_checkpoint = training_client.save_state(name=\"arithmetic-sft-checkpoint\").result()\n",
    "print(f\"Checkpoint saved to: {sft_checkpoint.path}\")\n",
    "print(\"\\nThis checkpoint contains:\")\n",
    "print(\"  - LoRA weights trained on arithmetic\")\n",
    "print(\"  - Adam optimizer state (momentum, variance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-header-md",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 2: Reinforcement Learning (RL)\n",
    "\n",
    "**Goal**: Load the SFT model and refine it using reward signals.\n",
    "\n",
    "**How RL differs from SFT**:\n",
    "- SFT: \"Here's the correct answer, learn it\"\n",
    "- RL: \"Try different answers, I'll tell you if you're right or wrong\"\n",
    "\n",
    "**RL Workflow**:\n",
    "```\n",
    "1. Generate multiple responses to each problem\n",
    "2. Compute rewards (1.0 = correct, 0.0 = wrong)\n",
    "3. Compute advantages (how much better than average?)\n",
    "4. Train: increase probability of good responses, decrease bad ones\n",
    "```\n",
    "\n",
    "**Why continue with RL after SFT?**\n",
    "- SFT teaches the format and basic capability\n",
    "- RL refines the model through exploration and reward optimization\n",
    "- Common pattern in real-world LLM training pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-checkpoint-md",
   "metadata": {},
   "source": [
    "### Step 8: Continue Training with RL\n",
    "\n",
    "We continue training the same model with RL. In production, you would typically:\n",
    "\n",
    "1. Save checkpoint: `training_client.save_state(name)`\n",
    "2. Load checkpoint: `service_client.create_training_client_from_state_with_optimizer(path)`\n",
    "\n",
    "For this demo, we continue with the same training client to show the SFT → RL transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-checkpoint-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with RL training using the same client\n",
    "# (In production, you would load from checkpoint for distributed training or resumption)\n",
    "\n",
    "rl_training_client = training_client  # Continue from SFT\n",
    "\n",
    "print(\"Continuing from SFT model...\")\n",
    "print(\"\\nThe model now has:\")\n",
    "print(\"  - SFT knowledge of arithmetic\")\n",
    "print(\"  - Ready for RL refinement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-env-md",
   "metadata": {},
   "source": [
    "### Step 9: Define the Reward Function\n",
    "\n",
    "In RL, we need a way to evaluate responses:\n",
    "- **Reward = 1.0**: Correct answer\n",
    "- **Reward = 0.0**: Wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rl-env-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rl_problem():\n",
    "    \"\"\"Generate harder multiplication problems for RL (10-199 vs SFT's 10-99).\"\"\"\n",
    "    a = random.randint(10, 199)\n",
    "    b = random.randint(10, 199)\n",
    "    return f\"What is {a} * {b}?\", str(a * b)\n",
    "\n",
    "def compute_reward(response: str, correct_answer: str) -> float:\n",
    "    \"\"\"Reward function: 1.0 if correct, 0.0 otherwise.\"\"\"\n",
    "    extracted = extract_answer(response)\n",
    "    return 1.0 if extracted == correct_answer else 0.0\n",
    "\n",
    "# Demo\n",
    "print(\"Reward function demo:\")\n",
    "print(\"RL problems: 10-199 * 10-199 (harder than SFT's 10-99 * 10-99)\")\n",
    "print()\n",
    "q, a = generate_rl_problem()\n",
    "print(f\"Question: {q}, Answer: {a}\")\n",
    "print(f\"  '{a}' → reward = {compute_reward(a, a)}\")\n",
    "print(f\"  '999' → reward = {compute_reward('999', a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-train-md",
   "metadata": {},
   "source": [
    "### Step 10: RL Training Loop\n",
    "\n",
    "The RL training loop:\n",
    "\n",
    "```\n",
    "for each step:\n",
    "    1. Sample multiple responses per problem (exploration)\n",
    "    2. Compute rewards for each response\n",
    "    3. Compute advantages = reward - mean_reward\n",
    "    4. Train with importance_sampling loss\n",
    "```\n",
    "\n",
    "**Key parameters**:\n",
    "- `GROUP_SIZE`: How many responses to sample per problem (more = better gradient estimate)\n",
    "- `temperature`: Controls randomness (higher = more exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rl-train-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mint import TensorData\n",
    "\n",
    "# RL Configuration\n",
    "NUM_RL_STEPS = 10\n",
    "BATCH_SIZE = 8\n",
    "GROUP_SIZE = 8\n",
    "MAX_TOKENS = 16\n",
    "RL_LEARNING_RATE = 2e-5\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "rl_metrics = []\n",
    "print(\"Starting RL training (continuing from SFT checkpoint)...\")\n",
    "print(f\"Config: {NUM_RL_STEPS} steps, {BATCH_SIZE} problems/batch, {GROUP_SIZE} samples/problem\")\n",
    "print()\n",
    "\n",
    "for step in range(NUM_RL_STEPS):\n",
    "    sampling_path = rl_training_client.save_weights_for_sampler(\n",
    "        name=f\"rl-step-{step}\"\n",
    "    ).result().path\n",
    "    \n",
    "    rl_sampling_client = service_client.create_sampling_client(\n",
    "        model_path=sampling_path,\n",
    "        base_model=BASE_MODEL\n",
    "    )\n",
    "    \n",
    "    problems = [generate_rl_problem() for _ in range(BATCH_SIZE)]\n",
    "    training_datums = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for question, answer in problems:\n",
    "        prompt_text = f\"Question: {question}\\nAnswer:\"\n",
    "        prompt_tokens = tokenizer.encode(prompt_text)\n",
    "        prompt_input = types.ModelInput.from_ints(prompt_tokens)\n",
    "        \n",
    "        sample_result = rl_sampling_client.sample(\n",
    "            prompt=prompt_input,\n",
    "            num_samples=GROUP_SIZE,\n",
    "            sampling_params=types.SamplingParams(\n",
    "                max_tokens=MAX_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                stop_token_ids=[tokenizer.eos_token_id]\n",
    "            )\n",
    "        ).result()\n",
    "        \n",
    "        group_rewards = []\n",
    "        group_responses = []\n",
    "        group_logprobs = []\n",
    "        \n",
    "        for seq in sample_result.sequences:\n",
    "            response_text = tokenizer.decode(seq.tokens)\n",
    "            reward = compute_reward(response_text, answer)\n",
    "            group_rewards.append(reward)\n",
    "            group_responses.append(seq.tokens)\n",
    "            group_logprobs.append(seq.logprobs if seq.logprobs else [0.0] * len(seq.tokens))\n",
    "        \n",
    "        all_rewards.extend(group_rewards)\n",
    "        \n",
    "        mean_reward = sum(group_rewards) / len(group_rewards)\n",
    "        advantages = [r - mean_reward for r in group_rewards]\n",
    "        \n",
    "        if all(a == 0 for a in advantages):\n",
    "            continue\n",
    "        \n",
    "        for response_tokens, logprobs, adv in zip(group_responses, group_logprobs, advantages):\n",
    "            if len(response_tokens) == 0:\n",
    "                continue\n",
    "            \n",
    "            full_tokens = prompt_tokens + list(response_tokens)\n",
    "            input_tokens = full_tokens[:-1]\n",
    "            target_tokens = full_tokens[1:]\n",
    "            \n",
    "            weights = [0.0] * (len(prompt_tokens) - 1) + [1.0] * len(response_tokens)\n",
    "            full_logprobs = [0.0] * (len(prompt_tokens) - 1) + list(logprobs)\n",
    "            full_advantages = [0.0] * (len(prompt_tokens) - 1) + [adv] * len(response_tokens)\n",
    "            \n",
    "            datum = types.Datum(\n",
    "                model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "                loss_fn_inputs={\n",
    "                    \"target_tokens\": TensorData.from_torch(torch.tensor(target_tokens, dtype=torch.int64)),\n",
    "                    \"weights\": TensorData.from_torch(torch.tensor(weights, dtype=torch.float32)),\n",
    "                    \"logprobs\": TensorData.from_torch(torch.tensor(full_logprobs, dtype=torch.float32)),\n",
    "                    \"advantages\": TensorData.from_torch(torch.tensor(full_advantages, dtype=torch.float32)),\n",
    "                },\n",
    "            )\n",
    "            training_datums.append(datum)\n",
    "    \n",
    "    avg_reward = sum(all_rewards) / len(all_rewards) if all_rewards else 0.0\n",
    "    accuracy = sum(1 for r in all_rewards if r > 0) / len(all_rewards) if all_rewards else 0.0\n",
    "    \n",
    "    if training_datums:\n",
    "        fwdbwd_future = rl_training_client.forward_backward(\n",
    "            training_datums,\n",
    "            loss_fn=\"importance_sampling\"\n",
    "        )\n",
    "        fwdbwd_future.result()\n",
    "        \n",
    "        optim_future = rl_training_client.optim_step(\n",
    "            types.AdamParams(learning_rate=RL_LEARNING_RATE)\n",
    "        )\n",
    "        optim_future.result()\n",
    "    \n",
    "    rl_metrics.append({\n",
    "        'step': step,\n",
    "        'avg_reward': avg_reward,\n",
    "        'accuracy': accuracy,\n",
    "        'num_datums': len(training_datums)\n",
    "    })\n",
    "    \n",
    "    print(f\"Step {step+1:2d}/{NUM_RL_STEPS}: Accuracy = {accuracy:5.1%}, Avg Reward = {avg_reward:.3f}\")\n",
    "\n",
    "print(\"\\nRL training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-test-md",
   "metadata": {},
   "source": [
    "### Step 11: Test the Final Model\n",
    "\n",
    "Compare the RL-refined model against the same test problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rl-test-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final RL model\n",
    "final_path = rl_training_client.save_weights_for_sampler(name=\"arithmetic-rl-final\").result().path\n",
    "final_client = service_client.create_sampling_client(\n",
    "    model_path=final_path,\n",
    "    base_model=BASE_MODEL\n",
    ")\n",
    "\n",
    "# Test on harder problems (10-199 range)\n",
    "rl_test_problems = [\n",
    "    (\"What is 123 * 45?\", \"5535\"),\n",
    "    (\"What is 67 * 189?\", \"12663\"),\n",
    "    (\"What is 156 * 78?\", \"12168\"),\n",
    "]\n",
    "\n",
    "print(\"Testing RL-refined model (10-199 range):\")\n",
    "print(\"=\" * 50)\n",
    "rl_correct = 0\n",
    "\n",
    "for question, correct in rl_test_problems:\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    prompt_input = types.ModelInput.from_ints(tokenizer.encode(prompt))\n",
    "    \n",
    "    result = final_client.sample(\n",
    "        prompt=prompt_input,\n",
    "        num_samples=1,\n",
    "        sampling_params=types.SamplingParams(\n",
    "            max_tokens=16,\n",
    "            temperature=0.0,\n",
    "            stop_token_ids=[tokenizer.eos_token_id]\n",
    "        )\n",
    "    ).result()\n",
    "    \n",
    "    response = tokenizer.decode(result.sequences[0].tokens)\n",
    "    extracted = extract_answer(response)\n",
    "    is_correct = extracted == correct\n",
    "    if is_correct:\n",
    "        rl_correct += 1\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response.strip()} (extracted: {extracted}, correct: {correct}) [{'PASS' if is_correct else 'FAIL'}]\")\n",
    "    print()\n",
    "\n",
    "print(f\"RL Accuracy: {rl_correct}/{len(rl_test_problems)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: SFT Loss\n",
    "axes[0].plot(range(1, len(sft_losses) + 1), sft_losses, 'b-o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Stage 1: SFT Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: RL Accuracy\n",
    "rl_steps = [m['step'] + 1 for m in rl_metrics]\n",
    "rl_accuracy = [m['accuracy'] for m in rl_metrics]\n",
    "\n",
    "axes[1].plot(rl_steps, rl_accuracy, 'g-o', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Stage 2: RL Training Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to training_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-final-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Save Final Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-final-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final RL checkpoint\n",
    "rl_checkpoint = rl_training_client.save_state(name=\"arithmetic-rl-final\").result()\n",
    "print(f\"Final checkpoint: {rl_checkpoint.path}\")\n",
    "\n",
    "print(\"\\nTo resume training later:\")\n",
    "print(f\"  client = service_client.create_training_client_from_state('{rl_checkpoint.path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "This tutorial demonstrated the complete MinT workflow:\n",
    "\n",
    "| Stage | Method | Loss Function | Purpose |\n",
    "|-------|--------|---------------|:--------|\n",
    "| 1 | SFT | `cross_entropy` | Teach multiplication with labeled examples |\n",
    "| 2 | RL | `importance_sampling` | Refine with reward signals |\n",
    "\n",
    "### Key API Methods\n",
    "\n",
    "```python\n",
    "# Setup\n",
    "service_client = mint.ServiceClient()\n",
    "training_client = service_client.create_lora_training_client(base_model=...)\n",
    "\n",
    "# Training\n",
    "training_client.forward_backward(data, loss_fn)  # Compute gradients\n",
    "training_client.optim_step(adam_params)          # Update weights\n",
    "\n",
    "# Checkpointing\n",
    "checkpoint = training_client.save_state(name)    # Save checkpoint\n",
    "resumed = service_client.create_training_client_from_state_with_optimizer(checkpoint.path)\n",
    "\n",
    "# Inference\n",
    "sampling_client = training_client.save_weights_and_get_sampling_client(name)\n",
    "sampling_client.sample(prompt, num_samples, sampling_params)\n",
    "```\n",
    "\n",
    "### Training Pipeline\n",
    "\n",
    "```\n",
    "SFT Training → save_state() → [optional: load checkpoint] → RL Training\n",
    "```\n",
    "\n",
    "This is the standard approach used in modern LLM training: SFT first to teach format and basic capabilities, then RL to refine through reward optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
